# a05 Aprendizaje Autom√°tico

# a05-02 Performance Evaluation

Pedro Larra√±aga, Concha Bielza

# Bondad de un clasificador
El rendimiento de un clasificador antes nuevos ejemplos
> Estimaci√≥n ‚Äúhonesta‚Äù  


Informaci√≥n de partida
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2015.13.05%202.png)

## Regiones de decisi√≥n
- Regi√≥n de decisi√≥n
	- 
- Frontera de decisi√≥n
	- Nos interesan fronteras muy flexibles
	- Las redes bayesianas generan hiperplanos de l√≠neas rectas
	- Una red con un par de capas ocultas, crean fronteras de ‚Äúcualquier‚Äù flexibilidad. (Curvas)

## Funci√≥n de P√©rdida / Riesgo

Funci√≥n de p√©rdida 0 o 1 , si fallas pagas una unidad, si aciertas no pagas 

- Riesgo esperado: valor atribuible para un clasificador y se calcula como una integral
	- Similar a una esperanza matem√°tica
	- `*R*(œÜ) = Ùè∞Ü *L*(*c*, œÜ(**x**))d*p*(**x**, *c*)`
- Funci√≥n emp√≠rica del riesgo
	- Asume que es uniforme para todos los puntos
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2015.20.21%202.png)


## Clasificaci√≥n binaria
> ¬øComo calcular la bondad de un clasificador?  

Tabla de incontingencia
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2015.23.17%202.png)

TP: true positives
FP: false positives
FN: false negatives
TN: true negatives 

Son valores enteros

Medidas de m√©rito


![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2015.24.44%202.png)

* Accuracy 
* Sensitivity or Recall 
* Specificity 
* Positive predictive value or Precision 
* Negative predictive value
* *F*1 -measure : Est√°n jugando con varios valores de la tabla. EC1, recuperaci√≥n de informaci√≥n. 
	* Media arm√≥nica entre la precisi√≥n y el recall.
	* F1 = 1 / 1/2(1/Prec + 1/Rec) = 2/ ((Prec+Rec)/Prec*Rec)
* Cohen‚Äôs kappa statistic : 
	* Cuanto m√°s cerca de 0 , m√°s cerca estaremos de la hip√≥tesis de independencia

> Media arm√≥nica: Inverso de la media aritm√©tica de los inversos  

> Hip√≥tesis de independencia  
> P(C>0) <->P(Fi(x)>0)  

> Lo normal es utilizar una medida  

Puedo entrenar un clasificado para maximizar un objetivo (cohen o accuracy o ‚Ä¶.)

## Matriz de coste
En vez de buscar maximizar un objetivo , no es equivalente a minimizar el coste total.
Solo es equivalente y la funci√≥n de p√©rdida es 0-1
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2015.38.19%202.png)


## Brier Score
Clasificadores probabil√≠sticos
> Es un score de calibraci√≥n  
Todas las probabilidades las llevan a 0 o a 1 -> Clasificador bien calibrado

Compara este clasificador con lo ideal.
- 0 est√° muy bien
- 2 es lo peor

Piramidal es + , se mide la distancia al punto euclideo (1, 0)
Interneurona es - , se mide la distancia al punto euclideo (0, 1)

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2015.41.37%202.png)


## Curva Roc
Se tiene en cuenta `FPR: False Positive Rate` y `TPR: True Positive Rate` al un√≠sono.

![](a05%20Aprendizaje%20Automa%CC%81tico/Foto%2026%20sept%202019%20154851%202.jpg)
Exijo como m√≠nimo 0.50 (√öltima columna)
+,+,-,+,
FPR -> cuando exijo 0.5 ; TPR -> 0.5
Si voy moviendo el umbral, ir moviendo desde 0.21 (el m√°s peque√±o) hasta el m√°s alto.

La curva roc , en realidad es una pol√≠gona, y es la uni√≥n de todos los puntos que hemos conseguido moviendo el umbral.

## Dominancia de pareto -> Multiobjetivo
B domina a A, porque B es mejor en los dos criterios TPR y FPR
C no domina ni a B ni a A
Cuantos m√°s criterios tenga, el **Frente de Pareto** ser√° muy grande
> No soy el mejor , y nadie es mejor que yo  
No hay nadie que me domine -> pertenece al frente de pareto

## Algoritmo ROC an√°lisis en una clasificaci√≥n binaria
‚Äútresshold?‚Äù aka th aka t
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2016.01.33%202.png)


Una cosa es la curva, pero lo que se utiliza es el √°rea bajo la curva
Cuanto m√°s area tiene, mejor es el clasificador


![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2016.03.43%202.png)

Ranki es la posici√≥n del positivo, ordenados de probabilidad de mayor a menor.
Lo ideal ser√≠a que la suma i - ranki sea 0, para que AUC(fi) sea 1

### Para problemas multi clase
hablamos de volumen

## M√©todos de Estimaci√≥n honesta
- Performance estimation
	- Single resampling
	- Multiple resampling

- Repetir y coger una muestra de tama√±o n 1 mill√≥n de veces
	- Calculo  el Par√°metro poblacional mu, media muestral 0.18
	- Hago un histograma
	- 

> Estimador insesgado (sin sesgo), es perfecto  
> La diferencia Lo que esperas estimar y la esperanza es 0  

Que el estimador tenga poca variabilidad. Varianza m√≠nima entre todos los  insesgados‚ÄúFrontera de tramel round‚Äù 
Estimador eficiente

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2016.22.29%202.png)

### Hold-out
	- Hago una partici√≥n de la muestra : Selecci√≥n aleatoria , sin reemplazamiento.
		- Para entrenar
		- Para testear
> no es una estrategia muy inteligente  

### k-fold cross-validation (Kurtz, 1948)
- Entreno con toda la muestra
- Para validar:
	- Separamos la muestra en k piezas
	- Los clasificadores intermedios, 
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2016.27.32%202.png)
> Si k= 10 -> La intersecci√≥n entre los clasificadores es del 80% (K-2)  

> El clasificador   
Estimaciones insesgadas, si k es muy alto 

> Si k coincide con el tama√±o de la muestra y solo testeamos  
> Mucha variabilidad, porque el resultado es 100 o 0  
> Leave-one out cross validation  

Stratified
Cuando las bases de datos est√°n dsevalanceadas, respecto a la variable clase
La clasificaci√≥n se har√° de forma estratificada, que en cada rodaja se mantenga el nivel de FP FN

El resultado del clasificador Acc = 

> Hoy en d√≠a es muy recomendado el k-fold para que tu clasificador sea entrenado con tu muestra entera  

## Multiple resampling-based estimation methods
### Repeated hold-out
Es mejor que que el hold out, porque no se han desperdiciado casos

### k-fold cross validation
10x10 cross validation , repetir 10 veces 
Dietterich (vaca sagrada) 5x2 k-fold no se usa

### Estimaci√≥n bootstrap
Extracciones con reemplazamiento
Cada muestra tendr√° el mismo tama√±o que la muestra original
Hasta conseguir mi muestra de tama√±o mil
Problemas:
- Probabilidad de que todos sean el mismo: `N(1/N)**N`
- Otro problema menos probable es que la muestra de test sean de la misma clase (todos + o todos negativos)

La probabilidad de no coger un elemento de la muestra es 1/e ~= 0.368
Efron, vaca sagrada
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2016.46.56%202.png)

La estimaci√≥n con bootstrap no es insesgada, pero s√≠ lo es en el l√≠mite. Asint√≥ticamente no sesgada.
Lo bueno que tiene una varianza muy peque√±a, en muestras peque√±as va bien

### Comparar clasificadores desde un punto de vista de test de hip√≥tesis
Comparamos los resultados uno o varios datasteis y uno o varios clasificadores en un test de hip√≥tesis

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2016.51.24%202.png)

Veremos el paired samples y el friedman test

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2016.52.18%202.png)

El p-valor bajo -> se rechaza la hip√≥tesis
‚ÄúEl p-valor debe 0.05 suele ser significativo y se rechaza la hip√≥tesis‚Äù
Generalmente se hace con el accuracy 

> no est√° bien hecho del todo  
> se critica porque el k-fold -2 son dependientes  

Por eso se corrige la no independencia

### Lo m√°s gen√©rico con 1 base de datos
Tengo 6 clasificadores y 1 √∫nica base de datos
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-09-26%20a%20las%2016.56.43%202.png)

Test de Nemenyi para encontrar 

### Lo m√°s gen√©rico con varias bases de datos
**Familywise** error, probabilidad de encontrar al menos un ti po de error in cualquier comparaci√≥n
Correcci√≥n Bonferroni

- [ ] Sentirnos libres para enviar un correo a concha y pedro si estamos interesados en participar en miner√≠a d e datos con problemas reales. 1 o dos becas. O hacer un doctorado en este tipo de cosas.




# a05-03 Arboles de clasificaci√≥n - 2.5 Classification trees

## Introducci√≥n
Concha Bielza, Pedro Larra√±aga

    x1 ‚Ä¶ xn | c
1
2
3

Variables | Predicci√≥n de clase (est√° enfermo, no)

Dos grandes grupos de clasificaci√≥n
- -> No probabil√≠sticos -> Los que te dan directamente la etiqueta
	- KNN : K vecinos m√°s pr√≥ximos
		- decides seg√∫n lo que te dicen los k vecinos
		- buscamos casos similares
	- √Årboles
	- Redes neuronales
	- ** soporte
- -> Te dan la etiqueta y la distribuci√≥n. Es mucho m√°s rico y normalmente te quedas con la probabilidad m√°s alta
	- Redes bayesianas
	- Regresi√≥n log√≠stica

> Cuando no son etiquetas y son n√∫meros reales, no es un problema de clasificaci√≥n, sino de regresi√≥n  

### ¬øQu√© es un √°rbol de clasificaci√≥n?
OJO! Los √°rboles de decisi√≥n -> es otra cosa en Sistemas de ayuda a la decisi√≥n. Aunque a veces se utilice el t√©rmino incorrectamente

- Muy usado y popular
- aproxima funci√≥n discreta-valor. La funci√≥n aprendida es representada por un √°rbol de decisi√≥n
- Robusto a datos con ruido
- Representa una disjunction of conjunctions de constantes en el atributo valor de instancias
	- Divisi√≥n de opciones. Colecci√≥n de ORS con ANDS
		- *(Outlook=Sunny*‚àß *Humidity=Normal)*‚à® *(Outlook=Overcast)*
‚à® *(Outlook=Rain*‚àß *Wind=Weak)*
- √°rbol Parendido puede ser re representado como colecciones de reglas if-then (intuitivo)
- 

### Representaci√≥n como un √°rbol
- cada nodo es una pregunta
- cada rama es un valor que toma la variable
- el nodo hoja son etiquetas de predicci√≥n
- 

### Ejemplo: Jugar al tenis
Outlook=Sunny, temperature=Hot,, humidity=High
Son variables continuas discretizables
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2015.21.47%202.png)

> XAI -> Explainable AI  
M√°s garant√≠as de que sea segura, justa, entender lo que dice el modelo para justificarlo.

> FSS -> Feature Subset selection  
Selecci√≥n de datos (variables) **relevantes**, no redundantes.
Queremos conseguir un conjunto peque√±o de variables que describa con precisi√≥n.
Jugar a quitar y poner
3 m√©todos:
- filtrado
- montura (wrapper)
- embedded

- Divisi√≥n de opciones. Colecci√≥n de ORS con ANDS
		- *(Outlook=Sunny*‚àß *Humidity=Normal)*‚à® *(Outlook=Overcast)*
‚à® *(Outlook=Rain*‚àß *Wind=Weak)*

Los aboles se pueden representar como una disjunction de conjuctions 

Navaja de okham, principio de parsimonia. Lo m√°s simple es mejor.

### Generaci√≥n de reglas
Los aboles se pueden representar como una disjunction de conjuctions 

**Inducci√≥n de reglas**
Del √°rbol pasamos a las reglas
De las reglas, es muy dif√≠cil pasar al √°rbol

> Determinar las fronteras de decision que nos ofrecen los √°rboles  

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2015.33.28%202.png)
> Cada nodo es una frontera (divisi√≥n) paralela a los ejes  

√°rboles oblicuos, preguntan por diferentes variables a la vez. [No lo vemos]
Nosotros vamos a preguntar por una variable cada vez  (> o <), por lo que nos crear√° l√≠neas divisoras paralelas a los ejes cardinales.

Classification trees: salidas discretas
	`CLS, ID3, C4.5, ID4, ID5, C4.8, C5.0`
√Årboles de regresi√≥n
	`CART, M5, M5P == M5‚Äô`

## √çndice


## Algoritmo b√°sico ID3
ID3 -> dicot√≥mico iterativo [Quinlan, 1986]
Particionan el dataset anterior en grupos por ramas

- Based on CLS (Concept Learning Systems) algorithm [Hunt et al., 1966], that only used binary attributes
- Greedy search strategy through the space of all possible classification trees Construct the tree top-down, asking: which attribute should be tested at the root?
- Answer by evaluating each attribute to determine how well it alone classifies training examples
- Best attribute is selected, a descendant of the root is created for each value of this attribute and the training examples are sorted to the appropriate descendant
- Repeat this process using the examples associated with each descendant node to select the best attribute at that point in the tree (always forward, searching among the not yet used attributes in this path)
- Stop when the tree correctly classifies the examples or when all attributes have been used
- Label each leaf node with the class of the examples

> Ganancia de informaci√≥n === Informaci√≥n mutua  
> Entrop√≠a de la clase, menos la entrop√≠a de la clse cuando conocemos Xi  
> Nos interesa que sea muy grande, porque implica que los valores es muy grande  

- et step: how to select which attribute to test at each node in the tree?
- We would like the most useful for classifying examples; that separates well the examples
- ID3 chooses mutual information as a measure of the worth of an attribute (maximize)
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2015.44.08%202.png)
- Expected reduction in entropy (uncertainty), caused by partitioning examples according to this attribute

### C√≥mo elegir un nodo ra√≠z
- Fijo esta variable.
	- ¬øQu√© pasa cuando vale 0 y cuando vale 1 ?
- M√°xima ganancia de valor, debe ir al principio del √°rbol, porque selecciona m√°s 

## Pasos para construir el √°rbol a partir de los datos
> !!! TODO: hacer los c√°lculos de las transparencias en casa.  

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2015.47.11%202.png)

1. Calcular la entrop√≠a de la clase para elegir el nodo ra√≠z
	1. lo aplicamos para cada una de las variables
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2015.48.34%202.png)
2. Outlook tiene la mayor `Ganancia de informaci√≥n === Informaci√≥n mutua`, por lo que ser√° nuestro nodo ra√≠z
	1. 
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2015.52.53%202.png)
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2015.53.13%202.png)
3. Volvemos a calcular la entrop√≠a para cada rama. Rain (solo 5 instancias), Sunny (otras 5 instancias).
	1. 
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2015.54.30%202.png)

### Observaciones y desventajas
> Es una heur√≠stica, por lo que seguramente no lleguemos a un √≥ptimo global.  

- Because of being greedy, it can lead to a local optimal solution rather than global
- Extension of ID3 to add a form of backtracking (post-pruning the tree)
- Because of using statistical properties of all the examples (in info gain), search is less sensitive to errors in data
- Extension of ID3 to handle noisy data by modifying its stopping criterion: create a leaf without perfectly fitting the data (label with majority)
- Complexity increases linearly with the number of training instances and exponentially with the number of attributes
- It has been shown that the selection of variables with more values is favored

### Deventajas
- Determine how deeply to grow the tree
- Choose an appropriate attribute selection measure Handling continuous attributes
- Handling training data with missing attribute values Handling attributes with differing costs
### Problema de Overfitting
Problema demasiado ajustado a los datos
Es mejor, buscar algo M√°s general, con menos variables

> Fallo intr√≠nseco debido al par√°metro de parada  
> ID3 lo va a hacer muy bien con el training y muy mal con el test  

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2016.01.43%202.png)

### Dos estrategias de Poda
Podamos el √°rbol para simplificarlo y que sea m√°s genial

- Prepoda: No se usa
- Postpoda:
	- Despu√©s de construir el √°rbol completo con el ID3 -> podamos
	- M√°s costoso, porque hacemos y luego deshacemos
	- Funciona de abajo a arriba
		- nos apuntamos cuanta precisi√≥n tiene el √°rbol antes de la poda
			- esta precisi√≥n se utiliza en un subconjunto de poda (que es diferente al de test)
		- miramos los sub√°rboles, un nodo (casi hoja) con sus hojas directas
			- conjunto de poda
		- y sustituimos el sub√°rbol y ponemos la etiqueta mayoritaria
		- comparamos la precisi√≥n y nos quedamos con las que mejora el resultado.


### Alterantivas que te√≥ricamente mejora, pero en realidad NO
Otros atributos de selecci√≥n de valores, diferentes a la precisi√≥n
Ejemplo: Fecha 
Para corregir las Variables con muchos valores (distribuci√≥n uniforme)  uniformemente repartidos, entrop√≠a altos. Hay que penalizarlos

Atributo: Grain Ratio
**Cociente de ganancia**

En la pr√°ctica no tiene tanto impacto, comparado con la poda

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2016.09.34%202.png)


## Variables continuas -> Las discreteamos
- Nodo A
	- < C
	- >= C
**C√≥mo encontrar C?**

Cambios de la clase

Ejemplo:
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2016.13.44%202.png)

Generamos cortes din√°micamente. Buscando puntos medios entre los cambios de clase de acuerdo a la variable continua. Pueden aparecer o desaparecer
- Ordenar
- Buscar cambios
- Elegir el punto medio
- Decidir con los atributos y las ‚Äú‚Äù¬øvariables?‚Äù‚Äù

### Datos que faltan MISSING
Si tenemos muchos datos, podemos eliminar las filas con esos campos
Pero lo m√°s habitual es ‚Äú*imputar*‚Äù , rellenar los missings
	- Hay SW que lo resuelve con estad√≠stica
	- Datos que cuestan recoger (datos de pacientes), o datasets peque√±os no merece la pena borrar filas.

> PROGRAMA PARA LA PR√ÅCTICA: Hueca  

- Soluci√≥n 1 -> Asignar la moda. El valor que m√°s se repite
- Soluci√≥n 2 -> Asignar la moda, pero solo los de mi misma etiqueta
- Soluci√≥n 3 -> Asignar toda la distribuci√≥n de probabilidad que hay en la columna

Estas estrategias se pueden hacer a nivel de dataset o durante una iteraci√≥n de ID3

### Incluir el coste en la selecci√≥n de medida de atributos
Dinero, tiempo, molestia
**Lower-cost attributes**
Queremos favorecer los atributos baratos
Y penalizar los m√°s caros
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2016.26.19%202.png)

Para normalizar los costes entre distintas variables, es una funci√≥n subjetiva, que no tenga unidad de medida.

## C4.5
Se llama C , porque fue escrito en C y 4.5 es la versi√≥n.

**Cociente de ganancia === gain ratio**

Reglas de post-poda

r1: si x1 = 1, x3=0, C=yes

Borramos contextos, ramas.
Esto produce que varias reglas se cumplan. Hay que calcular qu√© regla es la mejor.

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2016.29.33%202.png)



## MUNDO CONTINUO - **Continuous class variable (numeric prediction)** - ¬° REGRESI√ìN !
- Generalize these ideas for continuous regression functions
- Regression tree: at the leaves the value represents the response mean value of the instances that reach the leaf
- Adopted by the well-known CART system [Breiman et al., 1984] We approximate a non-linear function by a piecewise constant one Valid for any kind of attributes
- Model tree: at the leaves there is a linear regression model to predict the response of the instances that reach the leaf
- M5 algorithm [Quinlan, 1992] and M5‚Äô [Wang and Witten, 1997]
- Any kind of attributes, especially continuous Regression trees is a particular case of model trees


### Regresi√≥n !
Tengo puntos esparcidos en un eje. Hay que buscar la funci√≥n lineal representa esos puntos
Hip√≥tesis, modelo
`Y = B0+B1X+epsilon`
Estimaci√≥n: `Y = B0+B1X` (todos con ^)
Est√°n alineadas las medias.
Y esperanza

Lo que estima una regresi√≥n es una media en cada X

### √Årboles de regresiones
Abre opciones, y cuando llega a un nodo hoja -> llega a una variable respuesta (dependiente Y aka C)
Se usa en el SW CART
Es como una funci√≥n constante a trozos

### Model Tree
No da n√∫meros, da modelos, es mucho m√°s potente
Los nodos hojas son funciones

Ejemplo:
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2016.39.48%202.png)

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2016.41.15%202.png)

Cuando hueca implement√≥ el M5, le pusieron el ‚Äò prima, porque no era seguro que fuera el mismo.
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2016.41.45%202.png)

> ¬øQu√© criterio de bondad utilizamos para poner cada nodo?  
> Buscamos que no haya mucha varianza (desviaci√≥n t√≠pica) -> los valores son muy parecidos  

Nos interesa que los valores se vayan pareciendo para que la media acierte.
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2016.44.52%202.png)

- Nos interesa suavizar los pasos de unos hiperplanos a otros se pueden a√±adir las siguientes estrategias
	- Stop growing the tree when:
			- Only a few instances remain (‚â§ 4)
			- Class values of instances that reach a node vary very slightly (its SD is a small fraction of the overall SD with all data, e.g. 5 %)
	- Also pruning

Manejar atributos discretos X
Si una variable toma valores discretos (sexo, colores, etc)
Los convertimos en K-1 variables discretas
Sexo: H|M -> Hombre -> x1 = 0 | 1
Color: Rojo|Amarillo| Verde -> x1,x2 = 0,1 | 1,0 | 0,0
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2016.47.09%202.png)

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-03%20a%20las%2016.52.36%202.png)

## Conclusiones
- ID3 greedily selects the next attribute to be added in the tree
- Avoiding overfitting is important to have a tree that generalizes well ‚áí Pruning the tree
- Many extensions to ID3: handle continuous attributes, with missing values, other attribute selection measures, introduce costs
- Continuous class: model trees (M5) with linear regressions at the leaves

# a05-04 Bayesian Classifiers

20191010
2.3 Bayesian classifiers
Pedro Larra√±aga

## √çndice
1. Naive Bayes
2. Selective naive Bayes
3. Semi-naive Bayes
4. Tree augmented naive Bayes
5. Forest augmented naive
6. Bayes Superparent-one-dependence estimators k-dependence
7. Bayesian classifiers Bayesian network augmented naive
8. Bayes Markov blanket-based
9. Bayesian classifier
10. Unrestricted Bayesian classifiers
11. Bayesian multinets
12. Summary

> El clasificador m√°s utilizado en el grupo  

## Clasificador generativo
Si tengo N predictoras, las distribuci√≥n tendr√° N+1 variables

Redes bayesianas -> grafos

x1 -> C
x1 * x6 dados sus padres,

## Estimaci√≥n de par√°metros
> Siempre habr√° que estimar par√°metros independientemente de los   

M√°ximo veros√≠mil o aproximaci√≥n bayesiana
- MLE ->
	- ^P(X2=2 | C=1, X3=3)
- Estimaci√≥n bayesiana
	- estimaci√≥n de laplace cuando `√• = 1`
		- se puede usar m√°ximo veros√≠mil y aproximarla con laplace
	- Schrmann-Grassberger rule: `√•=/frac{1,/sub{R,j}}`


Smoothing suavizado de mis estimaciones y que ning√∫n par√°metro valga 0

## 1. Naive Bayes
- Desventaja
	- los n√∫meros a estimar no son independientes

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2015.49.34%202.png)

C√≥mo es la frontera de decisi√≥n? -> Es un hiperplano

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2015.50.05%202.png)

Pes√≥ como una losa saber que las fronteras eran hiperplanos, 36 a√±os en que surgiese una nueva propuesta de clasificador bayesiano
Fronteras de decisi√≥n demasiado simples
Funciona bien para bio inform√°tica, donde hay much√≠simas variables en una muestra peque√±a

### Naive Bayes con hidden variables
Se a√±aden variables ocultas
Regi√≥n de decisi√≥n es mucho m√°s simple.
- El modelo es m√°s complejo de entender
- El modelo es m√°s pesado para inducci√≥n

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2015.55.55%202.png)

## 2. Selective naive Bayes
Es el mismo naive bayes, pero no se usa todas las variables que nos llegan, sino que hacemos una selecci√≥n de variables. 
Nos quedamos con un subconjunto.

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.06.21%202.png)

- Filter: No involucro al clasificador
- Wrapper: Cuanto de bien lo est√° haciendo el clasificador

## 3. Semi-naive Bayes
Es el mismo naive bayes con **super nodo**
La variable puede tomar tantos valores como el producto de (X1,X3)

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.08.20%202.png)

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.08.35%202.png)

- comienza como una estructura vac√≠a, 
- Independientemente de los valores p(c|x1,x2,x3,x4) coincide con p(c)
- Tiene un porcentaje de bien clasificados que coincide con la clase mayor de c
	- 80% positivos, 20% negativos -> si siempre clasifica en positivo, acierta el 80% de casos
- Se busca le mejor opci√≥n entre: (se pueden usar en todos los pasos menos en el primero)
	- a√±adir: 
		- 
	- unir: 

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.11.40%202.png)

Crea super nodos indicando que esas variables est√°n muy relacionadas dado C


## 4. Tree augmented naive Bayes (TAN)
Lo m√°s usado para reemplazar naive bayes
Nuestro algoritmo va a encontrar ese √°rbol
Filter -> bas√°ndonos en cantidad de informaci√≥n mutua
	
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.17.04%202.png)

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.17.17%202.png)

√Årbol expandido de peso m√°ximo, donde el peso se calcula partir de cantidad de informaci√≥n mutua de dos variables predictoras dada una clase.
	Dos aristas con mayores pesos.

√Årbol no dirigido, para convertirle en dirigido, se coge un nodo al azar y se convierten sus  ___ en arcos

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.19.34%202.png)
No nos sirven c) y d) porque los arcos no pueden tener m√°s de un padre, para que no se invalide la condici√≥n de √°rbol.

- Necesitamos la probabilidad C
	- x1 | C
	- X2 | x1 y C
	- ‚Ä¶

El modelo no es complejo, porque todas las variables tienen solo 2 √°rboles.
C y otro nodo

## 5. Forest augmented naive
En vez de un √°rbol, se considera un bosque
> No es tan popular y no entramos en esto  

## 6. Bayes Superparent-one-dependence estimators - SPODE
SPODE
Tipo de estimador de una dependencia.
- En un ODEs
	- forma particular de un TAN
	- Cada variable predicadora, se permite que dependa de una variable
- SPODEs
	- se permite que todas las variables dependan de la misma variable
	- son interesante porque el AODE (abarajes the predictions of all qualified SPODEs) -> Metaclasificador

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.25.04%202.png)
Tiene suficientes casos como para poder estimar de ellos. Se necesitan al menos 30 casos.

## 7.  k-dependence Bayesian classifiers 
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.27.31%202.png)

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.30.55%202.png)

## 8. Bayesian network augmented naive
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.31.08%202.png)
Son propuestas de teor√≠a de informaci√≥n porque son filter.
Calculamos cosas que involucran dos o 3 variables y seg√∫n eso construyo nuestro modelo.

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.32.44%202.png)

> La complejidad no est√° limitada  

## 9. Bayes Markov blanket-based Bayesian classifier
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.34.56%202.png)

El manto de markov asocia cada variable con
- sus padres
- hijos 
- padres de los hijos.
subconjunto de variables del que realmente depende esa variable

C√≥mo determinar este manto
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.36.15%202.png)

> Es backward, eliminando variables hacia atr√°s  
Va eliminando las variables hasta aproximarse al MB(C)

Una variables es eliminada si la info que nos da, con respecto a las variables que ya hay es pr√°cticamente nula

Eliminamos variable a variable, manteniendo la distribuci√≥n de probabilidad.  Que se parezca lo m√°s posible cuando condiciones a todas las variables
Que se parezca seg√∫n la divergencia de ****

## 10. Unrestricted Bayesian classifiers
Muy parecido al manto de markov MB
> C puede tener padres igual que en MB  
Se aprenden con algoritmos gen√©ricos de Redes bayesianas, donde C juega igual que el resto de variables

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.39.52%202.png)

> La estructura la hemos conseguido con m√©todos gen√©ricos  


## 11. Bayesian multinets
Surgieron para representar independencias condicionales que dependen del contexto

> Se representa la Independencias condicionales dependientes del contexto.  
EJEMPLO: Una enfermedad (la tienes o no). Entre los sujetos adultos > 55 a√±os, las variables se interelacionan de forma distinta que en los j√≥venes.

Variable hip√≥tesis o variable **** (en el ejemplo es la edad)

La variable clase es ‚ÄúTienes enfermedad‚Äù ‚ÄúNo la tienes‚Äù
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2016.40.21%202.png)

m√°s ricos a nivel de sem√°ntica, dependientes del contexto

## 12. Summary
- Se puede evaluar con el ‚Äúbayes score‚Äù
- Resultados competitivos
- Descubren conocimiento, son explicables , transparentes etc. XIA

- Provides a posterior probability for each possible value of the class 
- Competitive results (accuracy, Brier, ROC) with the state of the art in supervised classifiers 
- Knowledge discovery from the structure of the Bayesian network 


## Referencias
- TAN -> N. Friedman, D. Geiger and M. Goldszmidt (1997). Bayesian network classifiers. Machine Learning, 29, 131-163
- HIPERPLANOS -> M. L. Minsky (1961). Steps toward artificial intelligence. Transactions on Institute of Radio Engineers, 49,8-30 

# a05-04 KNN K-Nearest Neighbors - Vecinos m√°s cercanos

20191010
2.2 K-NN-MUAI pdf
Pedro Larra√±aga

Paradigma vs modelo
Paradigma -> Gen√©rico,
Modelo -> algoritmo aplicado a una base de datos

Inducci√≥n del modelo, transducci√≥n del modelo

Muy intuitivo para el ser humano.

Mimetiza lo que hacemos cotidianamente

## √çndice
- The basic k-NN algorithm
- Theoretical results
- Variants of the basic k-NN
- Distance selection
- Prototype selection
- Instance-based learning

## The basic k-NN algorithm

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2015.06.49%202.png)


Zonas de influencia si fuera un 1-NN
No son fronteras de decisi√≥n
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2015.07.16%202.png)


- Advantages
		- 1 Able to learn complex decision boundaries
		- 2 No loss of information because there is no modeling (abstraction) phase
		- 3 A local method
		- 4 Few assumptions about the data
		- 5 Easily adapted as an incremental algorithm and also works when the input is a data stream of instances
		- 6 Easily adapted to regression problems
- Disadvantages
		- 1 High storage requirements
		- 2 Slow in classification time
		- 3 Sensitive to the value of k, the distance metric choice, the existence of irrelevant variables, and noisy data set
		- 4 There is no explicit model

## Theoretical results
	* **1**For unlimited quantities of data, the *k*-NN algorithm is guaranteed to yield an error rate no worse than twice the Bayes error rate (the minimum achievable error rate given the data distribution), when *k*increases as a function of the number of data points (Covert and Hart, 1967) 
	* **2**The repeated application of Wilson‚Äôs editing (a prototype selection method) will lead to the Bayes error rate (Devijer and Kittler, 1982) 
		* en el l√≠mite nos lleva al error de bayes
	* **3**Specified explicit decision boundaries for *k*-NN algorithms (Bremmer at al., 2005) 
		* Hay expresiones anal√≠ticas para las fronteras de decisi√≥n


## Variants of the basic k-NN
No se suele usar por defecto, se suele usar alguna variante.

### Weighting neighbors
The contribution of each neighbor depends on its distance to the query instance, with more weight
being attached to nearer neighbors
The weight wj of the j-th neighbor can be defined as a decreasing function h of its distance to the
instancetobeclassified,x:wj =h(d(xj,x))
The query instance will be assigned to the label with the largest total weight

### Weighting predictor variables
A weight to each predictor variable, that can be proportional to its relevance with respect to the
class variable
d(x, xi ) = Ùè∞Önj=1 wj Œ¥(xj , xji ), where wj is the weight assigned to variable Xj , and Œ¥(xj , xji )
measures the distance between the j-th components of x and xi
The weight wj may be proportional to the mutual information between Xj and C

> M√°s peso   

### Average distance
The distances of the neighbors to the query instance are averaged for each class label, and the label associated with the minimum average distance is assigned to the query

> En vez de pesar los casos considero lo k-vecinos cuales son positivos y negativos y calculo la distancia media y me quedo con la menor  

### k-NN with rejection
Demands some guarantees before an instance is classified
If the guarantees are not met, the instance remains unclassified until processed by another supervised classification algorithm according to a cascading procedure
A usual guarantee refers to the threshold for the most frequent class for the neighbors of the instance to be classified

Esto puede ser uno de los metaclasificadores de cascada

> No siempre voy a tomar una decisi√≥n.  
> Solo si tenemos muchas garant√≠as, que   
> Si k=10, si 9 o 10 son del mismo tipo  
> Los sin clasificar se dejan para otros clasificadores  


## Distance selection
### El problema de medir el aprendizaje. The metric learning problem
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-10%20a%20las%2015.22.15%202.png)

## Prototype selection
Estamos buscando un subconjunto de base de datos que sea consistente con la frontera de decisi√≥n
Consistencia de la frontera de decisi√≥n: Que la frontera de, es la misma que tendr√≠a con el resto de la base de datos

### Condensation, edition, hybrization

Si el segundo ejemplo es de la misma clase, no se tiene en cuenta.

- Prototype selection aims to output a representative smaller-sized training set than the original one that has a similar or even higher classification accuracy for new incoming data
- Ideally subsets that are both decision boundary consistent (the decision boundary found with the subset is identical to the boundary for the entire training set) and minimum consistent (the smallest subset of the training data that correctly classifies all original training data)
1.  **Condensed nearest neighbors algorithm (Hart, 1968)**
	- Iterative and starts by initializing the subset with a single training instance
	- In a second step, it classifies all remaining instances based on the subset, transferring any incorrectly classified samples to the subset
	- The last step repeatedly goes back to the second step until no transfers occur or the subset is full Incremental, order dependent, and neither minimal nor decision boundary consistent
	- The output subset will retain points that are closer to the decision boundaries, also called border points. At the same time, it will remove internal points
2. **Edition methods (Wilson, 1972)**: Quita los puntos que no est√°n de acuerdo. No hay orden. Se hace al final del todo.
	- Removes points that do not agree with most of their k nearest neighbors (noisy instances)
	- Smooth decision boundaries
	- Homogeneous groups
3. **Hybrid methods**
	- Combine edition and condensation strategies: first editing the training set to remove noise and smooth the decision boundary and then condensing the output of the edition to yield a smaller subset

## Instance-based learning
IBL -> Principios de los 90 y es el Machine Learning actual

- Instance-based learning (Aha et al., 1991) extends k-NN by providing incremental facilities, significantly reducing the storage requirements and introducing a hypothesis test to detect noisy instances
		1. **IB1**: Normalization of predictor variable ranges (avoiding, or reducing, the differences in variable weights for the Euclidean distance between two instances) and incremental instance processing
		2. **IB2**: Only instances near the decision boundaries are needed (reduce storage requirements). The set of informative instances is approximated as the training instances misclassified by IB1
		3. **IB3**: Noise-tolerant extension of IB2 that decides, using a simple test, which of the saved instances should be used to make classifications and which are believed to be noisy (these are discarded)

## Referencias
T.M. Covert, P.E. Hart (1967). Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13, 21-27
E. Fix, J.L. Hodges (1951). Discriminatory Analysis, Nonparametric Discrimination: Consistency Properties. USAF School of Aviation. Technical Report 4
D.L. Wilson (1972). Asympotic properties of nearest neighbor rules using edited data. IEEE Transactions on Systems, Man, and Cybernetics, 2(3), 408-421


Selecci√≥n de prototipos y de instancias


> Cambio de concepto  
> Concept ref   
> 	Virtual:   
> 	Real:   


# a05-05 Rule induction. 2.6 Rule-Induction-MUAI

- Pedro Larra√±aga
- 20191017
- 2.6 Rule-Induction-MUAI


Conocimiento
Motor de inferencia del sistema experto

## √çndice
- algoritmo 1R
	- muy simple en la que se basa el resto
	- trata de buscar reglas muy simples
- algoritmo AQR
- algoritmo CN2
- algoritmo RIPPER

## R1
Para cada valor de la clase
Selecciona una regla por variable
Se queda con la regla que m√°s porcentaje de clasificados tenga

## RIPPER

## IREP
- D es la uni√≥n entre D
	- GROW - crecer
	- PRUNE - podar
- Y la intersecci√≥n es vac√≠a.
	- D se participan en DGrow y DPrune
	- Con ejemplos + y -
		- para una clase binaria

Si es multi clase, se compara la clase X VS el resto

El conjunto de reglas

Una sucesi√≥n es una regla de literales.
Literal 

Hablamos de intersecci√≥n de literales

- **Cover** -> Cobertura -> son los ejemplos para los que esto es verdad
Cubre un conjunto limitado de casos.

IREP funciona de forma voraz hacia delante

IREP greedy forward y empieza en 0.

En esta regla no hay ning√∫n literal, y en cada paso se introduce un literal que maximice .
FOIL -> que vaya mejorando
Dejar√© de introducir literales cuando no encuentre ning√∫n literal hasta que no haga mejorar un **criterio FOIL**


![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-17%20a%20las%2015.24.41%202.png)


Regla R -> Regla actual, cubre 90 ejemplos de la base de datos, de los cuales positivos son 70 y negativos son 20

Consideramos 2 especializaciones de esa regla
R‚Äô = R + Literal1 -> cubre 80 ejemplos +50 y -20
R‚Äô‚Äô = R + Literal2 -> cubre 80 ejemplos

Ambas son dos especializaciones de R

> Buscamos la regla de  m√≠nima entrop√≠a  

IREP pone a competir reglas especializadas

La f√≥rmula da m√°s valor a la parte positiva que a la negativa

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-17%20a%20las%2015.34.20%202.png)


PODA
Criterio PRUNE
La regla es backward, siguiendo el orden inverso.
Primero pruebas con el √∫ltimo literal que ha entrado

ALGORITMO
Est√° en bucle es hasta que no quedan ejemplos positivos
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-17%20a%20las%2015.37.17%202.png)

# a05-05 Selecci√≥n de variables. 2.7 Feature subset selection

- Pedro Larra√±aga
- 20191017
- 2.7 Feature subset selection
- REVISAR LAS TRANSPARENCIAS

> No es PCA (Extracci√≥n de nuevas variables)  
> No es MDS (Multidimensional Scaling)  
> No es TSNE  

- Clasificaci√≥n supervisada
- Muchas variables
	- no son relevantes
	- otras son redundantes

En probabilidad se busca Omega(S) = 2^n
Si supera 50 o 100 , nos desborda

Ventajas Desventajas
- Reducci√≥n de dimensionalidad de datos
- Ayuda a los algoritmos de aprendizaje en su velocidad y efectividad
- mejora la precisi√≥n del clasificador
- Mejora de la interpretaci√≥n porque hay menos variables
- La desventaja es: computational burden 

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-17%20a%20las%2015.51.36%202.png)



## Aproximaci√≥n Filter
Juega con las propiedades intr√≠nsecas de los datos
> Para hacer una selecci√≥n filter no hace falta recurrir a ning√∫n clasificador  
> Esta selecci√≥n ser√° v√°lida para cualquier tipo de clasificador, porque no le involucramos  

De todos los subconjuntos (2^n) cu√°l es el mejor?

- Funciona muy bien en grandes datasets
	- una variable
- La selecci√≥n se realiza solo una vez
	- sirve para todo

- Filtrado 
	- univariante
		- a cada variable le aplico una funci√≥n en relaci√≥n a la clase
			- y elijo un threshold a partir del cual me quedo con todas las variables y tiro el resto
	- multivariante
		- eval√∫o cada subconjunto
			- el subconjunto depende de la b√∫squeda ü§∑üèª‚Äç‚ôÇÔ∏è



![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-17%20a%20las%2016.01.43%202.png)

PEDRO
>> Esto no s√© lo que es. Mejor os lo mir√°is en casa
>> Quer√≠a que nosotros ley√©ramos las transparencias y salgamos a presetar los algoritmos AQR y CN2 de `2.6 Rule-Induction-MUAI.pdf`
>> Me estoy liando, mejor os lo mir√°is en casa

>> Esto es un poco l√≠o [pag 26/48]
>> No, no lo entiendo, no lo entiendo
>> Me lo apunto, y os lo cuento la siguiente clase
>> Esto no funciona

>> Qu√© sentido tiene, si tiene alg√∫n sentido para vosotros
>> Lo estoy volviendo a hacer mal
>> Lo estoy liando
>> W1 se refiere solo a la primera  coordenada
>> Perd√≥n, estaba bien hecho
>> Perd√≥n , perd√≥n
>> No, estaba mal hecho, ahora est√° bien
>> No , i se est√° refiriendo al caso y la j se est√° refiriendo , digamos, a la variable



## Filter Model free

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-17%20a%20las%2016.20.12%202.png)
Para cada variable Xj tendremos una regla
Va seleccionar (filter) 
Cuanto de relevante es la variable
Si utilizamos esta variable para clasificar, cuantos √©xitos tendr√≠a

Cuanto m√°s grande es, m√°s seguro estoy de que es relevante

## Aproximaciones por envoltura
Subconjunto de variables y construyo el clasificador
Eval√∫o como de bueno es ese clasificador

> Tengo que construir un clasificador  
> Es costoso computacionalmente  

Heur√≠sticas 
>> La asignatura no va de heur√≠sticas de b√∫squedas , as√≠ que no voy a hacer muchos comentarios
>> No s√©, me voy a saltar
>> Pod√©is echar un ojo, pero creo que os lo van a contar mejor en otra asignatura
>> Os lo van a contar.
>> Voy terminando, aunque veo que faltan varias cosas

VNS -> 
Suponemos que hay una vecindad


## Estabilidad en la selecci√≥n de variables


# a05-06 Regresi√≥n Lineal
- Concha
- 20191024
- `2.4 Logistic regression.pdf`

- Modelos discriminativos
	- regresiones
- Modelos generativos
	- bayesianas


Una regresi√≥n es el modelo es una formula, que depende de unos par√°metros que vamos a estimarlos.

M√©todo de m√°xima verosimilitud -> likelihood

## Motivaci√≥n
### Objetivos
- Buscar la relaci√≥n entre una o m√°s variables productivas y dependientes.


## Approach

## Intuitions

## Expresiones : Ratio de riesgo
> En medicina es mejor dar un porcentaje para el riesgo, en vez de decir que el paciente est√° enfermo.  

## Expresiones: odds (posibilidades) and log it
- Regresi√≥n lineal:
	- √ü1 = es el cambio en e Y cuando en X1 se incrementa en 1
		- es lo que cambia el logit cuando la X1 se incrementa en 1
	- √ü2 = es el cambio en e Y cuando en X2 se incrementa en 1
	- ‚Ä¶
	- √ü0 es cuando todos los par√°metros son 0. Del patr√≥n 0.

Y = logit de œÄ = `ln(œÄ/1-œÄ)` = `√ü0+√ü1X1+‚Ä¶√ükXk`

**Escala de los logit**


## M√©todo de m√°xima verosimilitud
> > levantad la mano  
> > m√°s alto  
>   
Exponencial(lambda)
f(x) =lambdae
Funci√≥n de densidad  de muchas de las que hay

muestra de tiempo de vida de bombillas: x1, x2 ,x3 , sabemos que estas muestras vienen de una exponencial

Maximiza la verosimilitud = 

Primero vamos a calcular la funci√≥n de densidad de nuestra muestra
`f(x1,x2,‚Ä¶.,x5)`
Id√©nticas e independientemente distribuidas.
`f(x1) * f(x2) ...*f(x5)`
Lambda*e^-lambda*X1 * Lambda*e^-lambda*X2

Pasamos de verlo en funci√≥n de las x a verlo a una funci√≥n de lambda.

lambda^s

Funci√≥n de densidad en base del par√°metro. Nos dice que es m√°s veros√≠mil (m√°s se aproxima), cuanto m√°s alto sea el n√∫mero.

Hay que calcular la segunda derivada y evaluarla en los puntos que son 0 en la primera y ver que sea m√°ximo.

max de lambda L(lambda) es igual que calcular el m√°ximo de lambda en LN(L(lambda))
El m√°ximo se alcanza en el mismo punto, tomemos LN o no

Tomamos derivadas 

> Cuando queremos calcular  
> tambi√©n lo podemos usar para calcular el coeficiente de   
> Cogemos datos para la muestra  
> Calculamos la funci√≥n de verosimilitud   
> Maximizamos esa funci√≥n  
> Maximizamos el LN  
> tomamos derivadas para calcular el m√°ximo y en la exponencial nos saldr√° que Lambda es `1/la media de la muestra`  

> Podr√≠amos memorizar las funciones para calcular los par√°metros de las distribuciones dados unos par√°metros  

## Para maximizar el LN(L()) Newton-Raphson 

## Feature subset selection

**^ = Estimador = Variable aleatoria**

- Eliminar correlaciones en los predictores
	- Important to remove it, as done in linear regression
		- Se comparte en la regresi√≥n lineal
	- correlaciones altas , hacen una varianza alta

Salida de regresi√≥n log√≠stica:
- Term i = variables
- √üi = 
- SEi = error
- t-valor = √üi / SEi
	- estad√≠stico de **Wald**
	- se compara con la t-student
	- 
> Se hace mucho en regresi√≥n lineal para ver por separado si cada variable la podemos quitar  
> No es muy fiable en regresi√≥n log√≠stica  


Salidas de predicciones
Coeficiente de 
ASE = Asintotic Error -> 


### Incluir efectos no lineares
Se crean variables que son productos de dos variables.
Va en contra del principio de parsimonia
Principio de jerarqu√≠a , si quitas a la variable, no metas la pareja

### Tests de Hip√≥tesis
p valor = probabilidad de tener una muestra m√°s rara , y se rechazar√≠a la hip√≥tesis.


## T√©rminos
- Insesgado = Esperanza[media de X] = ¬µ
- varianza peque√±a = eficiencia
	- marca la estabilidad




**WEKA**
Modelo discrimanativo
aparecen funciones complejas, 
M√°xima verosimilitud -> 
Estrategias FSS
	tests espec√≠ficos para tener una estrategia backward

# a05-07 METACLASSIFIER - Metaclasificaci√≥n
- Concha
- 20191031
- `2.8 Metaclassifiers.pdf`

## PAPER para el POSTER
**PAPER Metodol√≥gico**
Art√≠culo de revista (no de conferencia).
Enviar un email con el paper seleccionado.

## Motivaci√≥n
- *Non free-lunch theorem:* Wolpert and MacReady 1996
- cada algoritmo converge mejor y falla en ciertas circunstancias
- Similar 


## Formas de caracterizar metaclasificadores
1. Distintos espacios de caracter√≠sticas (features)
	- un clasificador de voz, otro de imagen
	- cada clasificador hace una predicci√≥n
- Un combinador
	- que agrega datos

2. Un espacio √∫nico/com√∫n de caracter√≠sticas con diferentes clasificadores, d√≥nde cada clasificador son de:
	- diferentes tipos
	- entrenados con diferentes muestras
	- cambio de par√°metros

3. mejores set de componentes
	- no nos centramos tanto en la **precisi√≥n**
	- es mejor la **diversidad** , para que cometan diferentes errores
		- si todos dan la misma salida, no sirve tener/consultar varios

Resumen
- no nos centramos en optimizar la **precisi√≥n** de los clasificadores por separado
- son elegidos por **simpleza**
- cada uno es preciso en diferentes instancias, est√° **especializado** en subdominios
- esperamos que el **error decrezca** , porque su **varianza** decrecer√°
	- Bias-variance decomposition= total expected error of a classifier
	- Variance: source of error due to the particular training set used, inevitably finite and not fully representative of the actual population
	- Bias: measures how well the learning method matches the problem (the persistent error of the algorithm that can‚Äôt be eliminated even by taking an infinite number of training sets into account) 

# M√©todos b√°sicos
- Fusi√≥n de etiquetas 
	- voto de la mayor√≠a
		- con m√°s votos
	- mayor√≠a simple 
		- que haya m√°s de la mitad de votos de acuerdo 51%
	- voto de la mayor√≠a con un threshold
		- 80% de los clasificadores 
		- podr√≠amos quedarnos sin respuesta , sin etiqueta
			- `undefined`, no es fiable
			- o si hay un empate
	- voto de la Mayor√≠a ponderada
		- pesos por cada clasificador
		- hablan de la confianza, la bondad del clasificador
		- brier score, curva ro 
- Fusi√≥n de la salida de los valores continuos 
- Stacked generalization
- Cascading 

## Fusi√≥n de etiquetas (√Årboles, ‚Ä¶)
- L: n¬∫ de clasificadores (i)
- R: valores de las clases (j)
- x: un caso cualquiera de entrada
- dij(x): 1 o 0 , 1 si clasificador i clasifica a x como j , e.o.c. 0

Votos por mayor√≠a.
Para m√©todos no probabil√≠sticos.

## Fusi√≥n de salidas continuas (Regresiones, TAN). Salidas con probabilidad o una variable continua
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-31%20a%20las%2015.46.58%202.png)

Funci√≥n de soporte que saca un resumen de los resultados de cada clasificador.
- Media:
- Mediana: la utilizamos para evitar la influencia de valores muy en los extremos.
- M√°ximo, m√≠nimo
- Media  recortada: Trimmed 
	- jurados de competici√≥n (olimpiadas)
	- Se recortan los extremos, por ejemplo recorte de 20%, si tenemos 5 , quitar√≠amos el m√°s alto y el m√°s peque√±o y sobre el resto har√≠amos la media.
- Multiplicamos los valores: Ojo, si hay alg√∫n cero nos rompe el resumen.
- media generalizada: con nivel de optimismo del combinador

## M√©todo apilado de generalizaci√≥n (Wolpert 1992)
- Clasificadores de diferentes tipos
	- √°rbol
	- kan
	- reglas
	- ‚Ä¶
- En la capa 0 hay X clasificadores y su salida es la entrada de un √°rbol que ser√° el clasificador que de el resultado.
- En la capa 1 hay un clasificador en forma de √°rbol que intenta entender la decisi√≥n de cada clasificador. El stacking intenta aprender cuales son los m√°s fiables o qu√© relaci√≥n hay entre ellos.
	- Es brillante
	- Idea m√≠a, mejor que un √°rbol una red neuronal
	- Podr√≠amos equipararlo a un combinador inteligente en forma de √°rbol
- Tener en cuenta que el clasificador de la capa 1 deber√≠a se entrenado con labels generadas de forma honesta. con datos de testing y no de entrenamiento.

## Clasificador en cascada
Los clasificadores se esperan unos a otros
Las instancias se pueden explicar 

Al final se suele poner el k-nn, que suele ocuparse de las excepciones


# M√©todos avanzados
- **Bagging** = Bootstrap Aggregating (Breiman 1996)
	- Remuestreo
	- Probabilidad de ser elegido en la muestra de bootstrap ser√° 63%
		- se utiliza cuando la muestra es peque√±a
	- Vamos buscando la diversidad
	- Dividimos los datos de entrenamiento (training set) para entrenar cada clasificador
	- Todos los clasificadores tienen que ser de la misma clase. (Todo √Årboles, todo reglas, todo redes neuronales)
		- Los √°rboles, reglas, redes neuronales, son muy inestables, peque√±os cambios en las entradas
		- Tiene sentido usar s√≥lo clasificadores inestables
			- con peque√±os cambios 
	- Voto por la mayor√≠a
		- labels
		- porcentaje -> media y cogemos el mayor
- **Randomizaci√≥n**
	- **Random Forest** (Breiman, 2001) -> famoso por ganar muchas competiciones de Kaggle
	- Adem√°s de las filas, se cogen las columnas de forma aleatoria.
		- C√≥geme 5 y 8 filas al azar
	- Lo bueno de este m√©todo, se puede utilizar cualquier tipo de clasificador (estable o inestable).
- **Boosting**  (Freund Shapire, 1996)
	- Bagging es un M√©todo en paralelo
	- **Boosting** no, es en serie
		- Intenta aprender de los errores de los clasificadores anteriores
		- Muestreo selectivo: No todas las instancias tienen la misma probabilidad de aparecer en cada  clasificador.
			- las instancias mal clasificadas por el clasificador anterior tienen mayor probabilidad de se la muestra del siguiente clasificador
	- **AdaBoost** = Adaptive Boosting
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-31%20a%20las%2016.40.07%202.png)
- **Hybrid**
	- otras opciones que aparecen en el Weka
		- Lazy Bayesian Rules (Zheng y Webb, 2000)
			- Combinaci√≥n de Reglas con Naive Bayes
			- Concha no se acuerda
		- Naive Bayes Tree (Kohavi, 1996)
			- En sus hojas tiene naive bayes tree
		- Logistic Model Trees
			- es sus hojas tiene regresiones log√≠sticas

## Metaclasificadores en Weka
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-10-31%20a%20las%2016.46.02%202.png)

## Conclusiones
- Combining classifiers to try to find decision boundaries suitable for the problem 
	- Many possibilities
	- We lose interpretability 
- Conference *Internat. Workshop on Multiple Classifier Systems*(MCS) since 2000 has allowed organising the knowledge in this field, although it started in ‚Äô90 
- Active field of empirical research (still discussing about how and why it works) 
- Many webs, e.g. www.boosting.org, with much info 

Kuncheva (2004) 70% del libro es sobre combinadores.



# a05-08 Multi-Label Classification

- Pedro Larra√±aga
- 20191107
- `02-Supervised-classification/2.9 Multi-label classification.pdf` Pag 0

# Introducci√≥n. Categorizaci√≥n de texto
- ACM Computing Classification System 
	- Clasificar los documentos en las 11 
- Eur-Lex
	- Reducir de 200K a 5000 variables
- Aviation Safety Reporting System (ASRS)
	- 60 tipos de problemas que pueden aparecer en los vuelos
- ENRON
	- Fuga de emails de compa√±√≠a
	- Se usa como pagemark para realizar comparaciones
- Image understanding
	- problemas de visi√≥n, detectar distintos de objetos en una imagen
- NLP
	- detectar la edad, sexo, es nativo en el lenguaje?
- diagn√≥stico m√©dico
	- asdf
- Multiobjective regression - Multilabel classification
	- Predecir 
- Emotional categorization of music
	- tambi√©n se usa como benchmark. Datasetn Emotions
- Clasificaci√≥n de patentes internacional
	- 
- Bioinformatics
	- clasificaci√≥n de la funci√≥n de los genes
- Neurolog√≠a
	- Especia del animal, 
- Clasificaci√≥n demogr√°fica
	- De una cara, predecir la raza, edad, sexo
- Verificaci√≥n de identidad mediante selfies
- HIV Resistencia de drogas
	- es un coctel de f√°rmacos, y el cuerpo humano se hace resistente
	- predecir cual es el siguiente coctel de f√°rmacos que deber√≠amos recomendar al paciente

Muchas variables que son objetivos al mismo tiempo.
- Categ√≥ricas y binarias -> Mult-label classification
- Multiclass targets -> Multi-dimensional classification
	- nominal
		- matriz de costo por fallar de clase a a clase b
	- ordinal
		- necesariamente un fallo entre clases lejanas es un fallo m√°s grande que entre 
	- mezcla -> 
- Numerical targets -> multi-output regression

## Notaci√≥n
Pag 26




Hamming Loss -> Media  extendida de la diferencia sim√©trica a todos los casos y a todas las medidas

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-11-07%20a%20las%2015.42.07%202.png)


## M√©tricas de evaluaci√≥n
- 0/1 subset accuracy 
- Hamming Loss
- Information Retrival View
- One-Error
- Coverage
- Ranking Loss
	- Cuantas etiquetas no propias tiene delante 


## Methods for Learning Multi-label Classifiers

- Binary relevance
		- 
- Ranking via single-label learning
		- Heur√≠sticas
		- no puedo con el problema voy a hacer chapucillas
		- voy a considerar todos los casos que tienen solo una etiqueta
		- ignora casos con m√°s de una etiqueta
- Pairwise methods
		- **Ranking by pairwise comparison**
			- Voy a coger dos etiquetas a la vez
			- Para cada ejemplo, cada modelo
			- No selecciono cuando est√°n los dos , o cuando no est√° el segundo
			- Producimos ‚Äú6‚Äù nuevos modelos de clasificadores
				- para un nuevo ejemplo, se lo paso a los 6
				- Creamos un ranking por la cantidad de veces que responde un clasificador
				- Nos podemos quedar con cualquier clase y todos los que est√°n por encima de ese ranking
		- Calibrated label ranking
			- M√°s sofisticado que el pairwaise
			- Creamos una etiqueta virtual lambda 0
			- Sin perder ning√∫n ejemplo, contrastamos cada una de las etiquetas contra esa etiqueta virtual
			- 
- Methods that combine labels
		- Label powerset
		- Pruned sets
- Ensemble methods
		- RAkEL
		- Ensemble of pruned sets
- Identifying label dependencies
		- Correlation-based pruning of stacked BR models
		- Chi-Dep
		- Chi-Dep ensemble
		- Hierarchy of multi-label classifiers


## Adaptation Methods. Probabilistic Chain Classifier
- Boosting
- Rank-SVM
- Decision Tree
- Neural Network
- K -Nearest Neighbor
- Instance Based Using Mallows Model Instance Based + Logistic Regression
- Generative Parametric Mixture Model
- Probabilistic Chain Classifier
- Bayesian Network
	- Diagn√≥stico m√∫ltiple
	- Teorema de bayes
	- Multi-dimensional bayesian Network Classifier (MBC)
		- defino la estructura de la red bayesiana
		- 3 subgrafos
			- 1. Clase
			- 2. Puente
			- 3. Par√°metros predictoras
		- Distintas arquitecturas
			- Empty - Empty
				- no hay relaciones entre 
				- Previo filtrado para la extracci√≥n de variables predictoras basado en un **PCA**
				- Selecci√≥n basada en un GA para 
			- PGM - Tree-tree
			- HAIS
				- multietiqueta -> multiobjetivo
				- NSGA-II
					- Estructuras no dominadas
			- Bielza 2010 UPM-DIA Technical Report
				- DAG-DAG MBC



Saltar del multi-etiqueta al multi-dimensional pero en ordinal.






# a05-09 Unsupervised Classification - Clustering
- Concha
- 20191114
- `03-Unsupervised-classification/01-AA_Cluster_MUIA.pdf`


## √Åreas
- Marketing
	- clientes
- Biolog√≠a
	- 
- Seguros de coche
	- accidentes, fraude
- Web
	- documentos (basados en t√©rminos)
	- logs, grupos de usuario con accesos similares

## Tipos
- Jer√°rquicos : 
	- 
- No jer√°rquico
	- particional : k grupos prefijados, normalmente se barren varios k‚Äôs y nos quedamos con el que parece mejor
	- Probabil√≠stico: asume que 

## Pasos del clustering
1. Elegir la medida de similitud
2. Elegir una t√©cnica para ser usada (jer√°rquica o no)
3. Elegir el m√©todo / algoritmo de esa t√©cnica
	1. y el n√∫mero de k si corresponde
4. interpretaci√≥n los clusters formados (inferir las propiedades que dividen el objeto de acuerdo a la agrupaci√≥n)
	1. dar nombre

> Buscamos un grupo con cohesi√≥n interna y aislamiento externo  

### Similitud?
Cercan√≠a entre objetos.

Es un m√©todo exploratorio para generar un conjunto de hip√≥tesis. No para testearlos.
Exploraci√≥n multivariante de an√°lisis de datos es √∫til para scatter plots, chernoff‚Äôs faces, stars, awnrew‚Äôs curves.

- Las caras de chernoff
	- dibujo a nivel de instancia:
		- primera fila del dataset y lo convertimos en una cara
		- Cada n es un rasgo de la cara (ojos, nariz, boca, sombrero, color de pelo)
		- m√°s de 30 variables es complicado para el ojo humano
	- ~√∫til para estimar la cantidad de ks~ 
- Estrellas: Lo mismo que chernoff pero 
- Andrews‚Äôs curve : Cada instancia es una curva trigonom√©trica
	- se superponen las curvas
- **Plot de coordenadas paralelas**
	- el orden en el que pongas el las x afecta mucho a la representaci√≥n
	- [Parallel Coordinates in Matplotlib ‚Äì Ben Alex Keen](http://benalexkeen.com/parallel-coordinates-in-matplotlib/)

## Dos enfoques
1. Aglomerativo (Bottom-up) o ascendente
	1. el m√°s usado
	- hago n clusters
	- 
2. Divisivo (Top-down) o descendiente 


EJEMPLO
5 puntos en el espacio R3

## Distancias
- Single-linkage or nearest-neighbor: la distancia m√°s peque√±a. Al nodo m√°s cercano.
- Complete-linkage or farthest neighbor: la m√°s grande
- average-linkage: la media
- centroid: el punto intermedio (vector con la media coordenada a coordenada)
	- centroide entre 
- Ward‚Äôs: Suma de cuadrados y se trata de minimizar
	- se calcula el centroide del cluster
	- se calcula la suma de distancias al cuadrado de cada punto al centroide 
	- nos quedamos con el menor

Se genera un **Dendrograma**

**Scree plot** o regla del codo.
Diagrama de distancia a la que se ha alcanzado vs n¬∫ de paso . Podamos quedamos en el 

### Micro arrays de ADN
Nivel de expresiones de genes.
Instancias 100 vs genes 50K. Rojo sobre expresado, Verde infraexpresado

## Clustering particional
j=1,‚Ä¶N instancias
i=1,‚Ä¶,K clusters
rij={1 si xj se asigna a cli , 0 eoc}

mu sub i -> prototipo del cluster i = centroide

La funci√≥n objetivo es minimizar la suma de las distancias al cuadrado entre el elemento y el centroide al cuadrado, multiplicado por `rij` (en el caso que est√° en el cluster.

1. Seleccionas k semillas (los centroides de los clusters)
2. Asignamos cada objeto al cluster m√°s cercano
3. actualizar centroides
4. Repetir el proceso hasta que los centroides no cambien. (convergencia)

## K-mean - K-medias
### Forgy (1965)
Gran sensibilidad. Depende mucho de c√≥mo 
Los centroides no se actualizan mientras estamos calculando las r‚Äôs

### MacQueen (1967)
Cada vez que se asigna una r se calcula la mu


### Segmentaci√≥n de una imagen (Libro de Bishop 2006)
Representaci√≥n de una imagen en K colores.
Clustering de pixels

		R	G	B
Pixel 1 :
Pixel 2: 
‚Ä¶
Pixel N: 

## Preguntas
- Jer√°rquico o no
- qu√© m√©todo/algoritmo 
- qu√© distancia

- Jer√°rquico 
	- no hace falta preasignar la K
	- dendrograma es √∫til
	- La reasignaci√≥n no est√° permitida
	- sensible a la distancia escogida
- No jer√°rquico
	- sensitivo a la partici√≥n inicial
	- una buena opci√≥n es realizar el jer√°rquico para coger las semillas o elegir la K

### Qu√© algoritmos

### Dentro de 
K-medias funciona bien si son compactos
	nos permite una partici√≥n del espacio para usar como predictor
	**dirichlet‚Äôs tessellation**
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-11-14%20a%20las%2016.33.34%202.png)
	
Cuidado con las escalas 

Para evitar caer en un m√≠nimo local, podemos usar una estrategia Multi comienzo
La mediana es el elemento que est√° en el centro


## Distancias

Minkowski‚Äôs distance
P= 1 Distancia Manhattan - Manzanas de las ciudades
P = infinito Tcheychev


~Distancia de **Mahalanobis** = distancia de entre dos puntos que no afecta la escala.~


## C√≥mo evaluar la soluci√≥n del cluster
Hay m√°s de 30 medidas, pero evaluar con estas medidas no es riguroso.

- Distancias entre centroides
- Medida de Silhouette: numerador distancias dentro y denominador distancia externa. Silueta

Lo mejor (aunque es subjetivo ) La **validaci√≥n externa** , el experto.

### Interpretaci√≥n la soluci√≥n
- Poner un nombre a nuestro centroide ( 
- Una buena t√©cnica es comparar las diferencias entre centroides

**Lo que m√°s se usa en el departamento es contrastar con una clasificaci√≥n supervisada** 
Se coge el dataset y se completa con la etiqueta obtenida del clustering.
Y se convierte en un problema supervisado.
Aplicamos una t√©cnica: como √°rbol de decisi√≥n


PAG.42

# a05-10  COLONIAS DE HORMIGAS
- Alfonso
- 20191118
- 

## √çndice
* 1. Colonias de hormigas naturales 
* 2. La hormiga artificial 
* 3. El sistema de hormigas 
* 4. Otros sistemas de hormigas 
* 5. Aplicaciones 
* 6. Optimizaci√≥n multiobjetivo basada en CH 
* 7. Referencias 

## Colonias de hormigas naturales 
Algoritmos basados en c√≥mo se comportan las hormigas. Van dejando feromonas y calcular el camino m√≠nimo entre dos puntos.

Adem√°s de las feromonas se toma en cuenta la informaci√≥n de la distancia.

Se toma la decisi√≥n del camino para que no sea voraz y favorecer la b√∫squeda.


Probabilidad 
\tau = Lo que cuesta de ir de `i` a `a`

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-11-18%20a%20las%2017.15.01%202.png)

## La hormiga artificial
Mecanismo probabil√≠stico de construcci√≥n de soluciones al problema (un agente que imita a la hormiga natural).

Adem√°s tiene una informaci√≥n heur√≠stica 
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-11-18%20a%20las%2017.21.03%202.png) adicional.

### Problema del viajante (TSP) o cartero chino
Se puede resolver por esta MH.

Hormiga artificial debe elegir un camino con una probabilidad que depende de la cantidad de feromonas (Tau) y la informaci√≥n heur√≠stica que se tenga sobre esos arcos ().

La informaci√≥n heur√≠stica qu√© se tiene es la distancia entre dos nodos. Suele ser la inversa de la distancia.

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-11-18%20a%20las%2017.24.56%202.png)

√• = 0 -> solo nos fijamos en el heur√≠stico
√ü = 0 -> solo nos fijamos en las feromonas

### Actualizaci√≥n de la feromona

![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-11-18%20a%20las%2017.34.32%202.png)
![](a05%20Aprendizaje%20Automa%CC%81tico/Captura%20de%20pantalla%202019-11-18%20a%20las%2017.34.46%202.png)

1 - ro = cantidad de hormonas que permanece

## Otros sistemas de hormigas
* 1. Sistemas de hormigas elitistas(*AS**e*) 
	* a los arcos de la mejor soluci√≥n de los arcos en concreto, se les suma una cantidad proporcional al par√°metro e * el camino mejor (1 / distancia)
	* Estamos intensificando
	* mejores resultados que el no elitista
* 2. Sistemas de hormigas basadas en rankings (*AS**rank*) 
	* Tomaremos las `w-1` caminos mejores
	* Solo echamos hormonas a los primeros arcos y en funci√≥n de su ranking
	* (w-r) *feromonas
	* donde r es el ranking
	* valor t√≠pico de w = 6
	* adem√°s al mejor global se le suma una cantidad mayor w*feromonas
* 3. Sistemas de hormigas max-min (*MMAS*) 
	* Tenemos una cantidad m√≠nima y m√°xima de feromonas
* **4. Sistemas de colonias de hormigas (*ACS*)** 
	* **Explota m√°s la experiencia acumulada por las hormigas en la b√∫squeda que el AS**
	* Para el dep√≥sito y evaporaci√≥n de la feromona solo se considera la hormiga que gener√≥ la mejor soluci√≥n hasta ahora
	* una nueva actualizaci√≥n (local) de feromona basada en cada hormiga.
	* Cambia la probabilidad de ir de un nodo a otro cuando `q<q_0` va al nodo que haga m√°ximo el
	* valor t√≠pico de probabilidad de aceptaci√≥n es 0.9
	* esta probabilidad es m√°s **determinista**. 
	* tambi√©n difiere en c√≥mo se actualiza la feromona
		* lo que no se evapora * un par√°metro `ro=0.1` solo se aplica a los arcos de la mejor-global
* 5. Sistemas de hormigas mejor-peor  (*BWAS*) 
* 6. Estudio comparativo 
* 7. Sistemas de hormigas con b√∫squeda local 